import math
import torch.nn as nn
import torch
import pandas as pd
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
from model import ResCNN
from dataset import SpectralDataset
from WeightedMSEloss import WeightedMSELoss, build_peak_weight

# ========================= å‚æ•°é…ç½® =========================
device = "cuda" if torch.cuda.is_available() else "cpu"
train_dir = "D:\Desktop\dataset\guasnoise1-4080\\train"
val_dir   = "D:\Desktop\dataset\guasnoise1-4080\\val"
save_path = "/best_model.pth"
log_path  = "D:\Desktop\model\Res spectrum predict\\training_log.csv"
fig_path  = "D:\Desktop\model\Res spectrum predict\loss_curve.png"

num_epochs = 200
batch_size = 64
learning_rate = 1e-3
weight_decay = 1e-4

# ========================= æ•°æ®åŠ è½½ï¼ˆä¸æ¨ç†ä¿æŒä¸€è‡´ï¼šä¸åšå…‰è°±å½’ä¸€åŒ–ï¼‰ =========================
train_dataset = SpectralDataset(train_dir, normalize_spectra=False)
val_dataset   = SpectralDataset(val_dir,   normalize_spectra=False)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)

# ========================= æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ =========================
model = ResCNN().to(device)
core_weight = build_peak_weight(train_dataset, output_dim=76, alpha=3.0).to(device)
criterion_core = nn.MSELoss()
"""WeightedMSELoss(weight=core_weight)"""
optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)

# ======== é¢å¤–æ­£åˆ™ï¼šåŸºçº¿(DC)ä¸ä¸€é˜¶å¹³æ»‘(TV) ========
def tv_loss(y):
    return ((y[:,1:] - y[:,:-1])**2).mean()

def dc_baseline_loss(pred, target):
    return (pred.mean(dim=1) - target.mean(dim=1)).pow(2).mean()

lambda_tv = 0.02
lambda_dc = 0.10
lambda_l2_base = 1e-4   # åŸºçº¿ç³»æ•°å·²å†…åŒ–åœ¨æ¨¡å‹é‡Œï¼Œæ— æ³•ç›´æ¥å–ï¼›æ­¤å¤„ä¿ç•™é’©å­ä»¥ä¾¿æœªæ¥æ‰©å±•

# ========================= æ—¥å¿—è®°å½•å˜é‡ =========================
log = {"epoch": [], "train_loss": [], "val_loss": [], "learning_rate": []}
best_val = float("inf")
patience = 20
wait = 0

# ========================= è®­ç»ƒå¾ªç¯ =========================
for epoch in range(1, num_epochs+1):
    model.train()
    total = 0.0
    for images, spectra in train_loader:
        images = images.to(device).float()
        spectra = spectra.to(device).float()

        optimizer.zero_grad()
        outputs = model(images)

        loss_core = criterion_core(outputs, spectra)
        loss_tv   = tv_loss(outputs)
        loss_dc   = dc_baseline_loss(outputs, spectra)
        loss = loss_core + lambda_tv*loss_tv + lambda_dc*loss_dc
        loss.backward()
        optimizer.step()

        total += loss.item()

    train_loss = total / len(train_loader)

    # ===== éªŒè¯ =====
    model.eval()
    val_total = 0.0
    with torch.no_grad():
        for images, spectra in val_loader:
            images = images.to(device).float()
            spectra = spectra.to(device).float()
            outputs = model(images)
            loss_core = criterion_core(outputs, spectra)
            loss_tv   = tv_loss(outputs)
            loss_dc   = dc_baseline_loss(outputs, spectra)
            val_total += (loss_core + lambda_tv*loss_tv + lambda_dc*loss_dc).item()

    val_loss = val_total / len(val_loader)
    current_lr = optimizer.param_groups[0]["lr"]

    # ===== æ—¥å¿— =====
    log["epoch"].append(epoch)
    log["train_loss"].append(train_loss)
    log["val_loss"].append(val_loss)
    log["learning_rate"].append(current_lr)

    print(f"Epoch [{epoch:03d}/{num_epochs}] Train {train_loss:.4f} | Val {val_loss:.4f} | LR {current_lr:.6f}")

    # ===== ä¿å­˜æœ€ä¼˜ & è°ƒåº¦ & æ—©åœ =====
    if val_loss < best_val - 1e-6:
        best_val = val_loss
        wait = 0
        torch.save(model.state_dict(), save_path)
        print(f"âœ… Save model @ epoch {epoch} (val={val_loss:.6f})")
    else:
        wait += 1

    scheduler.step(val_loss)
    if wait >= patience:
        print("â¹ï¸ Early stop triggered.")
        break

# ========================= ä¿å­˜æ—¥å¿—å’Œå›¾åƒ =========================
df = pd.DataFrame(log)
df.to_csv(log_path, index=False)

plt.figure(figsize=(10, 6))
plt.plot(df["epoch"], df["train_loss"], label="Train Loss")
plt.plot(df["epoch"], df["val_loss"], label="Val Loss")
plt.xlabel("Epoch"); plt.ylabel("Loss"); plt.title("Training & Validation Loss")
plt.grid(True); plt.legend()
txt = (f"Epochs: {num_epochs}\nBatch: {batch_size}\nLR: {learning_rate}\nWD: {weight_decay}")
plt.gcf().text(0.98, 0.5, txt, fontsize=10, va='center', ha='right', bbox=dict(facecolor='white', alpha=0.7))
plt.tight_layout(); plt.savefig(fig_path); plt.close()

print(f"ğŸ“‘ æ—¥å¿—: {log_path}")
print(f"ğŸ“ˆ æ›²çº¿: {fig_path}")
print("ğŸ¯ è®­ç»ƒå®Œæˆï¼")
